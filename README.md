# ML Pipeline Speedrun: From Data to Deployment

**Date**: Saturday, August 09, 2025  
**Time Budget**: 2 hours  
**Goal**: Learn the complete ML pipeline by building a wine quality predictor

## ğŸ¯ Learning Objectives
- Understand end-to-end ML workflow
- Experience MLOps tools and practices
- Build a deployable ML service
- Learn ML-specific version control

## ğŸ—ï¸ Architecture Overview
```
Data â†’ Preprocessing â†’ Training â†’ Versioning â†’ Deployment â†’ Serving
 â†“         â†“              â†“           â†“            â†“           â†“
CSV    Feature Eng    sklearn     MLflow      Docker      FastAPI
       pandas         XGBoost      DVC                    Monitoring
```

## ğŸ“ Project Structure
```
ml-pipeline-speedrun/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Original data
â”‚   â””â”€â”€ processed/        # Cleaned, feature-engineered
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb # Quick EDA
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_prep.py      # Data pipeline
â”‚   â”œâ”€â”€ train.py          # Model training
â”‚   â””â”€â”€ serve.py          # FastAPI app
â”œâ”€â”€ models/
â”‚   â””â”€â”€ artifacts/        # Saved models
â”œâ”€â”€ mlruns/               # MLflow experiments
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## â±ï¸ Timeline & Tasks

### 0:00-0:20 | Data Preparation
- [ ] Load wine quality dataset from UCI/sklearn
- [ ] Exploratory analysis (distributions, correlations)
- [ ] Handle missing values, outliers
- [ ] Feature engineering (polynomial features, scaling)
- [ ] Create reproducible train/test split
- [ ] Save processed data with versioning

### 0:20-0:40 | Model Training
- [ ] Baseline model (Logistic Regression)
- [ ] Advanced model (Random Forest/XGBoost)
- [ ] Hyperparameter tuning (quick GridSearch)
- [ ] Cross-validation
- [ ] Log all experiments with MLflow
- [ ] Compare model performances

### 0:40-1:00 | Model Versioning
- [ ] Set up MLflow tracking server
- [ ] Log models, parameters, metrics
- [ ] Create model registry
- [ ] Initialize DVC for data versioning
- [ ] Tag best model for production
- [ ] Document model lineage

### 1:00-1:30 | Deployment
- [ ] Create FastAPI application
- [ ] Add prediction endpoint
- [ ] Add model metadata endpoint
- [ ] Write Dockerfile
- [ ] Build container image
- [ ] Test containerized app locally

### 1:30-2:00 | Serving & Monitoring
- [ ] Run production server
- [ ] Create simple web UI for predictions
- [ ] Add basic monitoring (latency, requests)
- [ ] Test with sample requests
- [ ] Document API with Swagger
- [ ] Push everything to GitHub

## ğŸ› ï¸ Tech Stack
- **Data**: pandas, numpy, scikit-learn
- **Training**: XGBoost, scikit-learn
- **Tracking**: MLflow, DVC
- **API**: FastAPI, pydantic
- **Deployment**: Docker, uvicorn
- **Monitoring**: Basic logging, prometheus (optional)

## ğŸ“Š Success Metrics
- âœ… Complete pipeline runs end-to-end
- âœ… Model achieves >85% accuracy
- âœ… API responds <100ms
- âœ… All code committed to GitHub
- âœ… Can reproduce results from scratch

## ğŸš€ Quick Start Commands
```bash
# Clone and setup
git clone <your-repo>
cd ml-pipeline-speedrun
pip install -r requirements.txt

# Run pipeline
python src/data_prep.py
python src/train.py
mlflow ui  # View experiments

# Deploy
docker build -t wine-predictor .
docker run -p 8000:8000 wine-predictor

# Test
curl -X POST http://localhost:8000/predict -d '{...}'
```

## ğŸ“ Key Learnings to Document
1. Why is data versioning different from code versioning?
2. How does MLflow differ from traditional logging?
3. What makes ML deployment unique?
4. How to ensure reproducibility?

## ğŸ”„ Future Improvements
- Add A/B testing capability
- Implement model retraining pipeline
- Add data drift detection
- Scale with Kubernetes
- Add GPU support

## ğŸ“š Resources
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [DVC Documentation](https://dvc.org/doc)
- [FastAPI Tutorial](https://fastapi.tiangolo.com/tutorial/)
- [Docker for Data Scientists](https://docker-curriculum.com/)
